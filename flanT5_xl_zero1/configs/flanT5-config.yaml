# FLAN-T5 Training Configuration for CNN/DailyMail

experiment:
  name: flan_t5_xl_cnn_dm
  output_dir: ./outputs/flan_t5_xl_cnn_dm

# Model configuration
model:
  name: "google/flan-t5-xl"
  pretrained_path: "google/flan-t5-xl"
  model_type: "t5"
  # Set to a checkpoint dir to resume; otherwise keep null
  resume_from_checkpoint: null

# Dataset configuration
data:
  dataset_name: "cnn_dailymail"
  dataset_config: "3.0.0"
  max_source_length: 512 #1024
  max_target_length: 128 # 128
  text_column: article
  summary_column: highlights  
  pad_to_max_length: false
  source_prefix: "summarize: "
  dataloader_workers: 4
  num_proc: 4 #8
  
  # For debugging - set to true to use subset
debug:
  use_subset: true #false
  subset_size: 8192
  train_subset_size: 8192
  valid_subset_size: 1024
  test_subset_size: 1024

# Training hyperparameters
training:
  num_epochs: 1 #2
  per_device_train_batch_size: 1 #4
  per_device_eval_batch_size: 1 #4
  gradient_accumulation_steps: 8 #8

  learning_rate: 1.0e-4 #3.0e-5
  weight_decay: 0.01
  warmup_steps: 20 #500   ## Rule of thumb: 5-10% of total training steps
  max_grad_norm: 1.0
  lr_scheduler_type: cosine #cosine is used for faster convergence #linear
  
  # Evaluation and saving
  #logging_steps: 10
  eval_steps: 10 #1000
  save_steps: 10 #1000
  save_total_limit: 1 #3
  evaluation_strategy: steps
  save_strategy: steps
  fp16: false
  bf16: true          # set true on A100 if your stack supports BF16
  seed: 42              # Reproducibility

  generation_max_length: 128 #256
  report_to: ["tensorboard"]
  load_best_model_at_end: false
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  gradient_checkpointing: false #true

  #memory, throughput and profiler config:
  enable_memory_profiler: True
  mem_log_steps: 20 # 5% usually
  enable_throughput_profiler: True
  logging_steps: 20 # 5% usually
  enable_pytorch_profiler: True
  profile_steps: 25
  wait_steps: 2
  warmup_steps: 3
  active_steps: 20
  repeat_steps: 1

# DeepSpeed configuration
deepspeed:
  enabled: true
  # Point these to your actual DS config JSON files:
  deepspeed_zero1_config: ./configs/ds_zero1.json
  deepspeed_zero2_config: ./configs/ds_zero2.json
  deepspeed_zero3_config: ./configs/ds_zero3.json
  deepspeed_zero3_offload_config: ./configs/ds_zero3_offload.json
  deepspeed_zero3_plusplus_config: ./configs/ds_zero3_plusplus.json
  deepspeed_zero3_plusplus_qwz_config: ./configs/ds_zero3_plusplus_qwz.json

# Paths
paths:
  cache_dir: "/pscratch/sd/m/megha89/hf_cache"
  output_dir: "/pscratch/sd/m/megha89/flant5-proj/zero1/outputs"
  trained_model_path: "/pscratch/sd/m/megha89/flant5-proj/zero1/outputs/final_model"
  resume_from_checkpoint: null

